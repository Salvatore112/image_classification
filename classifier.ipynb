{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/pasha/.cache/kagglehub/datasets/sumn2u/garbage-classification-v2/versions/8\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"sumn2u/garbage-classification-v2\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['trash', 'glass', 'battery', 'clothes', 'metal', 'plastic', 'cardboard', 'paper', 'biological', 'shoes']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from collections import defaultdict\n",
    "\n",
    "# Suppress warnings for better output\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "classes = os.listdir(os.path.join(path, \"garbage-dataset\"))\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Извлечение фичей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_yellow_amount(image):\n",
    "    \"\"\"Calculate the amount of yellow color in the image.\"\"\"\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    lower_yellow = np.array([20, 100, 100])\n",
    "    upper_yellow = np.array([30, 255, 255])\n",
    "    yellow_mask = cv2.inRange(hsv_image, lower_yellow, upper_yellow)\n",
    "    yellow_amount = cv2.countNonZero(yellow_mask)\n",
    "    return yellow_amount\n",
    "\n",
    "def calculate_silver_amount(image):\n",
    "    \"\"\"Calculate the amount of silver color in the image.\"\"\"\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    lower_silver = np.array([0, 0, 200])\n",
    "    upper_silver = np.array([180, 25, 255])\n",
    "    silver_mask = cv2.inRange(hsv_image, lower_silver, upper_silver)\n",
    "    silver_amount = cv2.countNonZero(silver_mask)\n",
    "    return silver_amount\n",
    "\n",
    "def calculate_parallel_lines(image):\n",
    "    \"\"\"Calculate the amount of parallel lines in the image.\"\"\"\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray_image, 50, 150, apertureSize=3)\n",
    "    \n",
    "    # Detect lines using Hough Transform\n",
    "    lines = cv2.HoughLines(edges, 1, np.pi / 180, threshold=100)\n",
    "    \n",
    "    if lines is not None:\n",
    "        return len(lines)  # Return the number of detected lines\n",
    "    return 0  # No lines detected\n",
    "\n",
    "def detect_cylinders(image):\n",
    "    \"\"\"Detect cylindrical shapes in the image.\"\"\"\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    blurred_image = cv2.GaussianBlur(gray_image, (9, 9), 2)\n",
    "\n",
    "    # Detect circles using Hough Transform\n",
    "    circles = cv2.HoughCircles(blurred_image, cv2.HOUGH_GRADIENT, dp=1, minDist=30,\n",
    "                               param1=100, param2=30, minRadius=10, maxRadius=100)\n",
    "\n",
    "    if circles is not None:\n",
    "        return circles.shape[1]  # Return the number of detected circles\n",
    "    return 0  # No circles detected\n",
    "\n",
    "def calculate_reflection(image):\n",
    "    \"\"\"Calculate the amount of bright areas in the image to detect reflections.\"\"\"\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Threshold to find bright areas\n",
    "    _, bright_mask = cv2.threshold(gray_image, 200, 255, cv2.THRESH_BINARY)\n",
    "    reflection_amount = cv2.countNonZero(bright_mask)\n",
    "    return reflection_amount\n",
    "\n",
    "def get_features(image):\n",
    "    \"\"\"Get a set of features (yellow amount, silver amount, parallel lines, cylindrical shapes, reflections) from the image.\"\"\"\n",
    "    yellow_amount = calculate_yellow_amount(image)\n",
    "    silver_amount = calculate_silver_amount(image)\n",
    "    parallel_lines = calculate_parallel_lines(image)\n",
    "    cylinder_count = detect_cylinders(image)\n",
    "    reflection_amount = calculate_reflection(image)\n",
    "    return yellow_amount, silver_amount, parallel_lines, cylinder_count, reflection_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15834, 284114, 140, 15, 303454)\n"
     ]
    }
   ],
   "source": [
    "# Feature extraction testing\n",
    "test_im = cv2.imread(path + \"/garbage-dataset/cardboard/cardboard_25.jpg\")\n",
    "print(get_features(test_im))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение классификатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4361163820366856\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     battery       0.29      0.36      0.32       145\n",
      "  biological       0.40      0.50      0.45       138\n",
      "   cardboard       0.46      0.41      0.44       311\n",
      "     clothes       0.56      0.76      0.65       886\n",
      "       glass       0.33      0.35      0.34       455\n",
      "       metal       0.23      0.14      0.17       163\n",
      "       paper       0.38      0.24      0.30       263\n",
      "     plastic       0.32      0.21      0.25       327\n",
      "       shoes       0.36      0.30      0.33       317\n",
      "       trash       0.39      0.28      0.33       157\n",
      "\n",
      "    accuracy                           0.44      3162\n",
      "   macro avg       0.37      0.36      0.36      3162\n",
      "weighted avg       0.41      0.44      0.42      3162\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the dataset\n",
    "base_path = path + \"/garbage-dataset\"\n",
    "classes = ['trash', 'glass', 'battery', 'clothes', 'metal', 'plastic', 'cardboard', 'paper', 'biological', 'shoes']\n",
    "\n",
    "# Initialize lists to hold features and labels\n",
    "features = []\n",
    "labels = []\n",
    "test_images = []  # List to hold paths of test images\n",
    "\n",
    "# Loop through each class and extract features from images\n",
    "for class_name in classes:\n",
    "    class_path = os.path.join(path + \"/garbage-dataset/\", class_name)\n",
    "    image_names = os.listdir(class_path)\n",
    "\n",
    "    # Split the images into training and testing sets (80% train, 20% test)\n",
    "    train_images, test_images_class = train_test_split(image_names, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Process training images\n",
    "    for image_name in train_images:\n",
    "        image_path = os.path.join(class_path, image_name)\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        if image is not None:\n",
    "            yellow_amount, silver_amount, parallel_lines, cylinder_count, reflection_amount = get_features(image)\n",
    "            features.append([yellow_amount, silver_amount, parallel_lines, cylinder_count, reflection_amount])\n",
    "            labels.append(class_name)\n",
    "\n",
    "    # Collect test images for manual testing\n",
    "    for image_name in test_images_class:\n",
    "        test_images.append(os.path.join(class_path, image_name))\n",
    "\n",
    "# Convert features and labels to a DataFrame\n",
    "df = pd.DataFrame(features, columns=['yellow_amount', 'silver_amount', 'parallel_lines', 'cylinder_count', 'reflection_amount'])\n",
    "df['label'] = labels\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = df[['yellow_amount', 'silver_amount', 'parallel_lines', 'cylinder_count', 'reflection_amount']]\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Function to classify a new image\n",
    "def classify_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is not None:\n",
    "        yellow_amount, silver_amount, parallel_lines, cylinder_count, reflection_amount = get_features(image)\n",
    "        feature = np.array([[yellow_amount, silver_amount, parallel_lines, cylinder_count, reflection_amount]])\n",
    "        prediction = classifier.predict(feature)\n",
    "        return prediction[0]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: glass\n"
     ]
    }
   ],
   "source": [
    "# Classifier testing (runs classifier for a specific image)\n",
    "result = classify_image(path + '/garbage-dataset/metal/metal_2370.jpg') \n",
    "print(\"Predicted class:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class counts for test images containing trash:\n",
      "Class: metal, Count: 4\n",
      "Class: glass, Count: 43\n",
      "Class: trash, Count: 60\n",
      "Class: plastic, Count: 14\n",
      "Class: battery, Count: 15\n",
      "Class: shoes, Count: 18\n",
      "Class: cardboard, Count: 8\n",
      "Class: clothes, Count: 20\n",
      "Class: biological, Count: 4\n",
      "Class: paper, Count: 4\n",
      "Predicted class counts for test images containing glass:\n",
      "Class: trash, Count: 25\n",
      "Class: shoes, Count: 86\n",
      "Class: metal, Count: 18\n",
      "Class: glass, Count: 210\n",
      "Class: clothes, Count: 129\n",
      "Class: plastic, Count: 35\n",
      "Class: battery, Count: 23\n",
      "Class: paper, Count: 18\n",
      "Class: biological, Count: 26\n",
      "Class: cardboard, Count: 43\n",
      "Predicted class counts for test images containing battery:\n",
      "Class: glass, Count: 30\n",
      "Class: paper, Count: 3\n",
      "Class: biological, Count: 5\n",
      "Class: clothes, Count: 26\n",
      "Class: battery, Count: 77\n",
      "Class: plastic, Count: 4\n",
      "Class: shoes, Count: 25\n",
      "Class: metal, Count: 5\n",
      "Class: cardboard, Count: 11\n",
      "Class: trash, Count: 3\n",
      "Predicted class counts for test images containing clothes:\n",
      "Class: clothes, Count: 804\n",
      "Class: shoes, Count: 32\n",
      "Class: paper, Count: 50\n",
      "Class: cardboard, Count: 30\n",
      "Class: glass, Count: 58\n",
      "Class: metal, Count: 10\n",
      "Class: battery, Count: 18\n",
      "Class: biological, Count: 27\n",
      "Class: plastic, Count: 36\n",
      "Class: trash, Count: 1\n",
      "Predicted class counts for test images containing metal:\n",
      "Class: paper, Count: 11\n",
      "Class: metal, Count: 37\n",
      "Class: battery, Count: 9\n",
      "Class: clothes, Count: 50\n",
      "Class: trash, Count: 5\n",
      "Class: glass, Count: 33\n",
      "Class: biological, Count: 13\n",
      "Class: shoes, Count: 22\n",
      "Class: cardboard, Count: 9\n",
      "Class: plastic, Count: 15\n",
      "Predicted class counts for test images containing plastic:\n",
      "Class: clothes, Count: 125\n",
      "Class: cardboard, Count: 33\n",
      "Class: battery, Count: 8\n",
      "Class: plastic, Count: 89\n",
      "Class: glass, Count: 57\n",
      "Class: trash, Count: 16\n",
      "Class: paper, Count: 20\n",
      "Class: shoes, Count: 26\n",
      "Class: biological, Count: 13\n",
      "Class: metal, Count: 10\n",
      "Predicted class counts for test images containing cardboard:\n",
      "Class: cardboard, Count: 166\n",
      "Class: battery, Count: 10\n",
      "Class: trash, Count: 5\n",
      "Class: glass, Count: 42\n",
      "Class: metal, Count: 10\n",
      "Class: clothes, Count: 71\n",
      "Class: shoes, Count: 19\n",
      "Class: biological, Count: 11\n",
      "Class: plastic, Count: 15\n",
      "Class: paper, Count: 16\n",
      "Predicted class counts for test images containing paper:\n",
      "Class: paper, Count: 92\n",
      "Class: clothes, Count: 130\n",
      "Class: glass, Count: 43\n",
      "Class: plastic, Count: 12\n",
      "Class: cardboard, Count: 21\n",
      "Class: trash, Count: 7\n",
      "Class: metal, Count: 10\n",
      "Class: battery, Count: 9\n",
      "Class: shoes, Count: 9\n",
      "Class: biological, Count: 3\n",
      "Predicted class counts for test images containing biological:\n",
      "Class: cardboard, Count: 11\n",
      "Class: clothes, Count: 33\n",
      "Class: biological, Count: 104\n",
      "Class: glass, Count: 22\n",
      "Class: shoes, Count: 16\n",
      "Class: plastic, Count: 6\n",
      "Class: trash, Count: 3\n",
      "Class: battery, Count: 4\n",
      "Class: paper, Count: 1\n",
      "Predicted class counts for test images containing shoes:\n",
      "Class: glass, Count: 59\n",
      "Class: paper, Count: 8\n",
      "Class: metal, Count: 13\n",
      "Class: clothes, Count: 75\n",
      "Class: cardboard, Count: 21\n",
      "Class: plastic, Count: 15\n",
      "Class: biological, Count: 32\n",
      "Class: shoes, Count: 137\n",
      "Class: battery, Count: 22\n",
      "Class: trash, Count: 14\n"
     ]
    }
   ],
   "source": [
    "# Classifier testing (runs classifier for the remaining images)\n",
    "def test_classify(test_images, target_class_name):\n",
    "    predictions = defaultdict(int)\n",
    "    for test_image in test_images:\n",
    "        if target_class_name in test_image:  \n",
    "            predicted_class = classify_image(test_image)  \n",
    "            predictions[predicted_class] += 1 \n",
    "    return dict(predictions)  \n",
    "\n",
    "for class_name in classes:\n",
    "    results = test_classify(test_images, class_name)\n",
    "    print(\"Predicted class counts for test images containing \" + class_name + ':')\n",
    "    for class_name, count in results.items():\n",
    "        print(f\"Class: {class_name}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка гипотез"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
